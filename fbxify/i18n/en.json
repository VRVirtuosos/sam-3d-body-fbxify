{
  "app": {
    "title": "SAM 3D Body ‚Üí Unity FBX",
    "heading": "üßç‚Äç‚ôÇÔ∏è SAM 3D Body ‚Üí Unity Humanoid FBX",
    "program_tab": "Program",
    "about_tab": "About",
    "features_title": "Features",
    "features": [
      "Generate Unity Humanoid-compatible FBX files from a single image",
      "Generate multi-frame animations from MP4 videos",
      "3D mesh + skeleton + **animation**"
    ],
    "usage_title": "Usage",
    "usage": [
      "Use the Pose Estimation tab to upload an image or MP4 video",
      "Click the \"Estimate Pose\" button",
      "After pose estimation completes, switch to the Generate FBX tab",
      "(Optional) Download the generated JSON for future use, tuning refinement, etc.",
      "Click the \"Generate FBX\" button",
      "Download the generated FBX ‚Üí Import to Unity",
      "The following are optional features:",
      "Upload your own bboxes and camera intrinsics file from gta-link, boxmot, colmap",
      "For video, you may or may not want root motion for your armature - check the box if you do",
      "Apply standard mocap-style smoothing to the animation via Refinement. Enabled by default with some customizable settings."
    ]
  },
  "ui": {
    "pose_tab_label": "Pose Estimation",
    "fbx_tab_label": "Generate FBX",
    "profile": "üìä Armature Model",
    "input_file": "üìÅ Image/Video File",
    "use_bbox": "üë• Use BBOX File",
    "bbox_file": "üìÅ BBOX File",
    "num_people": "üë• Number of People",
    "missing_bbox_behavior": "On Missing Frame",
    "missing_bbox_behavior_info": "What to do when bbox data is missing for a frame: Run Detection (use num_people to detect) or Skip Frame (skip pose estimation for that frame)",
    "tracking": {
      "mode_label": "üß≠ Tracking Mode",
      "mode_info": "Choose how people are tracked during pose estimation",
      "mode_bbox": "Use BBOX File",
      "mode_count": "Closest N People",
      "mode_inference": "Inference Tracking",
      "mode_inference_bbox": "Inference Tracking + BBOX File",
      "config_title": "Inference Tracking Config",
      "enabled": "Enable Tracking",
      "enabled_info": "Turn inference tracking on/off (only applies in Inference Tracking mode)",
      "thresholds_title": "Association Thresholds",
      "max_gap_frames": "Max Gap Frames",
      "max_gap_frames_info": "Max frame gap before a tracklet is closed",
      "merge_max_gap_frames": "Merge Max Gap Frames",
      "merge_max_gap_frames_info": "Max gap allowed when merging tracklets",
      "min_tracklet_length": "Min Tracklet Length",
      "min_tracklet_length_info": "Minimum detections required to keep a tracklet",
      "min_similarity": "Min Similarity",
      "min_similarity_info": "Minimum similarity score to link detections",
      "shape_distance_threshold": "Shape Distance Threshold",
      "shape_distance_threshold_info": "Distance threshold for shape_params similarity",
      "cam_distance_threshold": "Camera Distance Threshold",
      "cam_distance_threshold_info": "Distance threshold for pred_cam_t similarity",
      "min_cam_similarity": "Min Cam Similarity",
      "min_cam_similarity_info": "Reject matches when cam similarity is below this (e.g. 0.01 rejects cam=0 teleports). Only when Use pred_cam_t is on.",
      "min_pose_similarity": "Min Pose Similarity",
      "min_pose_similarity_info": "Reject matches when pose similarity is below this (e.g. 0.3 rejects ~30% pose agreement). Only when Use Pose Aux is on. 0 = no gate.",
      "shape_maturity_frames": "Shape Maturity (frames)",
      "shape_maturity_frames_info": "Trust shape only after this many detections in the tracklet. Below this, match on pose and cam only (age-aware re-ID).",
      "high_shape_override_cam": "High shape overrides cam",
      "high_shape_override_cam_info": "When tracklet is mature and shape similarity is very high, allow match even if cam is low (same person re-emerged elsewhere).",
      "high_shape_threshold": "High Shape Threshold",
      "high_shape_threshold_info": "Shape similarity threshold for the override above (e.g. 0.95).",
      "pose_distance_threshold": "Pose Distance Threshold",
      "pose_distance_threshold_info": "Distance threshold for pose auxiliary similarity",
      "iou_distance_threshold": "IoU Distance Threshold",
      "iou_distance_threshold_info": "Distance threshold for bbox IoU (requires bbox_xywh)",
      "weights_title": "Similarity Weights",
      "shape_weight": "Shape Weight",
      "shape_weight_info": "Weight for shape_params similarity",
      "cam_weight": "Camera Weight",
      "cam_weight_info": "Weight for pred_cam_t similarity",
      "pose_weight": "Pose Weight",
      "pose_weight_info": "Weight for pose auxiliary similarity",
      "iou_weight": "IoU Weight",
      "iou_weight_info": "Weight for bbox IoU similarity",
      "features_title": "Features",
      "use_shape_params": "Use Shape Params",
      "use_pred_cam_t": "Use pred_cam_t",
      "use_pose_aux": "Use Pose Aux",
      "use_bbox_iou": "Use BBox IoU",
      "export_frame_assignments": "Export Frame Assignments",
      "export_tracklet_detections": "Export Tracklet Detections",
      "export_mot_bboxes": "Export MOT BBoxes",
      "bg_title": "Background Filtering",
      "bg_enabled": "Enable Background Filtering",
      "bg_enabled_info": "Optional: reduce background crowds/occluders during inference tracking. Default off.",
      "bg_detection_gates_title": "Detection Gates (per-frame)",
      "bg_min_bbox_height_px": "Min bbox height (px)",
      "bg_min_bbox_height_px_info": "Drop detections with bbox height below this. 0 disables.",
      "bg_min_bbox_area_px2": "Min bbox area (px¬≤)",
      "bg_min_bbox_area_px2_info": "Drop detections with bbox area below this. 0 disables.",
      "bg_depth_max_z": "Max depth z (pred_cam_t)",
      "bg_depth_max_z_info": "Keep detections with pred_cam_t[2] <= this. 0 disables.",
      "bg_keep_nearest_z_quantile": "Keep nearest z quantile",
      "bg_keep_nearest_z_quantile_info": "Per frame: keep the nearest fraction by z (e.g. 0.7 keeps nearest 70%). 0 disables.",
      "bg_auto_size_title": "Auto Size Cutoff",
      "bg_size_auto_method": "Auto size method",
      "bg_size_auto_method_info": "none/percentile/otsu/gmm2. Uses bbox height/area. Applied per frame.",
      "bg_size_feature": "Size feature",
      "bg_size_feature_info": "Which bbox size proxy to use for size-based filtering.",
      "bg_size_percentile": "Drop small percentile",
      "bg_size_percentile_info": "For percentile method: drop the smallest fraction (e.g. 0.2 drops smallest 20%). 0 disables.",
      "bg_tracklet_scoring_title": "Tracklet Scoring (stable subjects)",
      "bg_tracklet_score_enabled": "Enable tracklet scoring",
      "bg_tracklet_score_enabled_info": "Scores tracklets by stability/persistence and can drop likely background tracks.",
      "bg_tracklet_score_threshold": "Tracklet score threshold",
      "bg_tracklet_score_threshold_info": "Keep tracklets with score >= threshold. 0 disables.",
      "bg_min_tracklet_frames_for_scoring": "Min frames for scoring",
      "bg_min_tracklet_frames_for_scoring_info": "Tracklets shorter than this are treated as low-confidence (often occluders).",
      "bg_w_length": "Weight: length",
      "bg_w_length_info": "Foreground score weight for tracklet duration/coverage.",
      "bg_w_size": "Weight: size",
      "bg_w_size_info": "Foreground score weight for median bbox size.",
      "bg_w_size_stability": "Weight: size stability",
      "bg_w_size_stability_info": "Foreground score weight for bbox size stability over time (IQR).",
      "bg_w_centering": "Weight: centering",
      "bg_w_centering_info": "Foreground score weight for staying near the group center.",
      "bg_auto_roi_title": "Auto ROI (computed)",
      "bg_auto_roi_enabled": "Enable Auto ROI",
      "bg_auto_roi_enabled_info": "Build a moving ROI from the stable subjects to suppress background detections.",
      "bg_auto_roi_window_frames": "Auto ROI window (frames)",
      "bg_auto_roi_window_frames_info": "Window size used to estimate ROI center/radius.",
      "bg_auto_roi_point": "ROI point",
      "bg_auto_roi_point_info": "Use bbox bottom-center (best for standing on a floor) or bbox center.",
      "bg_auto_roi_mad_k": "ROI radius MAD factor",
      "bg_auto_roi_mad_k_info": "Radius = MAD * k (robust spread). Higher = larger ROI.",
      "bg_auto_roi_min_radius_px": "Min ROI radius (px)",
      "bg_auto_roi_min_radius_px_info": "Minimum ROI radius to avoid over-filtering when few detections exist.",
      "bg_auto_roi_smoothing_alpha": "ROI smoothing alpha",
      "bg_auto_roi_smoothing_alpha_info": "EMA smoothing for ROI center/radius (0 = no updates, 1 = no smoothing).",
      "bg_refine_second_pass": "Second-pass refinement",
      "bg_refine_second_pass_info": "If enabled with Auto ROI, reruns association using the Auto ROI gate to reduce track explosion.",
      "load_config": "Load Tracking Config",
      "save_config_btn": "Save Tracking Config",
      "save_config": "Tracking Config File"
    },
    "fov_method": "üì∑ FOV Estimation Method",
    "fov_method_info": "Default: Built-in, runs every frame | File: Load from cameras.txt | Sample: Average from images",
    "fov_file": "üìÅ Camera Intrinsics File (COLMAP cameras.txt or MoGe format)",
    "sample_number": "üìä Sample Number",
    "sample_number_info": "Number of images to sample and average for FOV estimation",
    "precision": "üéõÔ∏è Precision",
    "precision_info": "FP32 for best quality, BF16/FP16 for faster inference",
    "frame_batch_size": "Frame batch size",
    "frame_batch_size_info": "Run pose model once per N frames (1 = per-frame). 4 or 8 can speed up video. Higher = more GPU memory.",
    "detection_batch_size": "Detection batch size",
    "detection_batch_size_info": "When no BBOX file: number of images per detector batch (1 = per-frame). Higher can speed up detection.",
    "auto_run": "‚ö° Auto-Run Generate FBX (after Pose Estimation)",
    "use_root_motion": "üß≠ Apply Root Motion",
    "auto_floor": "üß≠ Auto-Floor",
    "auto_floor_info": "Offset the average pred_cam_t height to 0 so subjects stand above the floor",
    "include_mesh": "üé≠ Include Mesh",
    "include_extrinsics": "üì∑ Include Extrinsics",
    "use_personalized_body": "üë§ Use Personalized Body",
    "use_personalized_body_info": "Generate a personalized body mesh from estimation data. If unchecked, uses default LOD body.",
    "lod": "üìä LOD",
    "lod_info": "Level of Detail for mesh (0-6)",
    "outlier_removal_percent": "üìä Outlier Removal (%)",
    "outlier_removal_percent_info": "Percentage of outliers to remove from each tail (0-50)",
    "extrinsics_sample_rate": "üìä Extrinsics Downsampling Rate",
    "extrinsics_sample_rate_info": "0 = auto (round(frame_count / extrinsics_count))",
    "extrinsics_scale": "üìè Extrinsics Scale",
    "extrinsics_scale_info": "Scales the extrinsics translation components",
    "extrinsics_invert_quaternion": "üîÅ Invert Extrinsics Rotation",
    "extrinsics_invert_quaternion_info": "Treat qvec as camera‚Üíworld instead of world‚Üícamera",
    "extrinsics_invert_translation": "üîÅ Invert Extrinsics Translation",
    "extrinsics_invert_translation_info": "Treat tvec as camera‚Üíworld instead of world‚Üícamera",
    "extrinsics_file": "üìÅ Extrinsics File (COLMAP images.txt)",
    "estimate_pose_btn": "üéØ Estimate Pose",
    "step_1_estimate_pose": "Estimate Pose",
    "generate_fbx_btn": "üöÄ Generate FBX",
    "step_2_generate_fbx": "Generate FBX",
    "options_title": "Options",
    "estimation_options_title": "Estimation Options",
    "fbx_options_title": "FBX Options",
    "pose_json_file": "üìÑ Pose Estimation Outputs",
    "pose_json_file_info": "Upload a pose estimation JSON file, or it will be auto-populated after estimation",
    "output_files": "üì¶ Generated Files",
    "developer_options": "Developer Options",
    "pose_dev": {
      "output_tracking_bbox": "Output Tracking BBOX",
      "run_bbox_detection_now": "Export detection to MOT",
      "run_bbox_detection_now_info": "Run detection on the current input (video or image) and save an MOT file. Use with **BBOX File** to skip re-detection when estimating.",
      "bbox_detection_output": "Detection file (MOT)",
      "estimation_file_rerun_tracking": "Estimation file (re-run tracking)",
      "rerun_tracking_btn": "Re-run tracking",
      "step_through_console": "Step-through (pause each frame in console)",
      "debug_start_frame": "Start debugging on frame",
      "debug_start_frame_info": "Frame index where step-through and per-frame debug begin (0 = from first frame). Use to jump near a problem frame."
    },
    "export_personalized_body_obj": "üß™ Export Personalized Body Obj",
    "graph_refinement": "üìà Graph Refinement",
    "create_camera": "üì∑ Create Camera",
    "camera_zoom": "üì∑ Camera Zoom",
    "camera_scene": "üéûÔ∏è Camera Scene",
    "cancel_current_jobs": "üõë Cancel Current Jobs",
    "cancel_current_jobs_info": "Stop any running Estimate Pose or Generate FBX job (requires queue).",
    "pose_cli_generator_title": "Pose Estimation CLI",
    "pose_cli_generator_info": "Create a command template for pose estimation. Replace <INPUT_FILE> and other placeholders before running.",
    "pose_generate_cli_btn": "üß∞ Generate Pose CLI",
    "pose_cli_command_label": "Pose CLI Command",
    "pose_cli_command_info": "Template command for pose estimation runs",
    "fbx_cli_generator_title": "Generate FBX CLI",
    "fbx_cli_generator_info": "Create a command template for FBX generation. Replace <INPUT_FILE> and other placeholders before running.",
    "fbx_generate_cli_btn": "üß∞ Generate FBX CLI",
    "fbx_cli_command_label": "FBX CLI Command",
    "fbx_cli_command_info": "Template command for FBX generation runs",
    "refinement": {
      "title": "Refinement",
      "enabled": "Enabled",
      "enabled_info": "Enable refinement processing (occurs after generation)",
      "load_config": "Load Config",
      "load_config_info": "Upload a JSON configuration file",
      "save_config": "Save Config",
      "save_config_info": "Download current configuration as JSON",
      "save_config_btn": "Save Configuration",
      "global_settings": "Refinement Global Settings",
      "sections": {
        "root": "Root",
        "hands": "Hands",
        "fingers": "Fingers",
        "head": "Head",
        "legs": "Legs",
        "arms": "Arms",
        "default": "Default"
      },
      "max_pos_speed": "Max Positional Speed",
      "max_pos_speed_info": "Maximum positional speed (units/sec) for spike detection",
      "max_pos_accel": "Max Positional Acceleration",
      "max_pos_accel_info": "Maximum positional acceleration (units/sec¬≤) for spike detection",
      "max_ang_speed_deg": "Max Angular Speed (Degrees)",
      "max_ang_speed_deg_info": "Maximum angular speed (deg/sec) for spike detection",
      "max_ang_accel_deg": "Max Angular Acceleration (Degrees)",
      "max_ang_accel_deg_info": "Maximum angular acceleration (deg/sec¬≤) for spike detection",
      "method": "Method",
      "method_info": "Smoothing filter method",
      "method_one_euro": "One Euro",
      "method_ema": "EMA",
      "method_butterworth": "Butterworth",
      "cutoff_hz": "Cutoff Frequency (Hz)",
      "cutoff_hz_info": "Cutoff frequency for EMA/Butterworth filters",
      "one_euro_min_cutoff": "One Euro Min Cutoff (Hz)",
      "one_euro_min_cutoff_info": "Minimum cutoff frequency for One Euro filter",
      "one_euro_beta": "One Euro Beta",
      "one_euro_beta_info": "Speed coefficient for One Euro filter",
      "one_euro_d_cutoff": "One Euro Derivative Cutoff (Hz)",
      "one_euro_d_cutoff_info": "Derivative cutoff frequency for One Euro filter",
      "root_cutoff_xy_hz": "Root Cutoff XY (Hz)",
      "root_cutoff_xy_hz_info": "Cutoff frequency for horizontal (XY) root motion",
      "root_cutoff_z_hz": "Root Cutoff Z (Hz)",
      "root_cutoff_z_hz_info": "Cutoff frequency for vertical (Z) root motion",
      "interpolate_missing_keyframes": "Interpolate Missing Keyframes",
      "interpolate_missing_keyframes_info": "When enabled, missing frames (where no people were detected) will be interpolated using slerp for rotations and linear interpolation for vectors. When disabled, missing frames are skipped during refinement.",
      "use_foot_planting": "Use Foot Planting",
      "use_foot_planting_info": "Enable foot planting to adjust root motion based on foot contact, reducing jitter and creating more natural walking/running motion",
      "foot_planting": {
        "title": "Foot Planting",
        "foot_contact_velocity_threshold": "Foot Contact Velocity Threshold (m/s)",
        "foot_contact_velocity_threshold_info": "Foot is considered 'planted' if its velocity is below this threshold",
        "foot_contact_min_height": "Foot Contact Min Height (m)",
        "foot_contact_min_height_info": "Foot must be below this height (Y coordinate) to be considered in contact",
        "contact_smoothing_window": "Contact Smoothing Window",
        "contact_smoothing_window_info": "Number of frames to use for smoothing contact detection (reduces flickering)",
        "blend_factor": "Blend Factor",
        "blend_factor_info": "How much to blend foot-based root adjustment with original root motion (0-1, higher = more adjustment)",
        "root_smoothing_window": "Root Smoothing Window",
        "root_smoothing_window_info": "Number of frames to use for smoothing the adjusted root motion",
        "use_mid_foot": "Use Mid Foot",
        "use_mid_foot_info": "Use mid_foot (average of heel/toes) instead of ankle for foot position"
      }
    }
  },
  "progress": {
    "processing_keyframes": "üì¶ Processing keyframes...",
    "processing_person": "üì¶ Processing person...",
    "reexporting": "üîÑ Re-exporting from saved results...",
    "applying_refinement": "Applying refinement to estimation results...",
    "preparing_fbx_data": "Preparing FBX data...",
    "preprocessing_complete": "Pre-processing complete",
    "refining_person": "Refining person {person_index} of {total_people}",
    "refinement_complete": "Refinement complete",
    "applying_poses": "Applying poses: {frame_num}/{total_frames}",
    "estimating_frame": "Estimating frame {frame_index} of {total_frames}"
  },
  "errors": {
    "error_occurred": "An error occurred ({error_type}): {error_msg}",
    "error_occurred_no_msg": "An error occurred: {error_type}",
    "bbox_file_required": "BBOX file must be provided when 'use bbox' is enabled",
    "num_people_required": "Number of people must be greater than 0",
    "camera_intrinsics_required": "Camera intrinsics file must be provided when FOV method is 'File'",
    "fov_estimator_required": "FOV estimator must be loaded to use 'Sample' method. Please ensure FOV estimator is initialized.",
    "pose_json_file_required": "Please provide a pose estimation JSON file"
  }
}
